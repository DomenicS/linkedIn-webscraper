{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Domen\\OneDrive\\Projects\\linkedIn-webscraper\n"
     ]
    }
   ],
   "source": [
    "# change direction to main folder to use path from config.json\n",
    "\n",
    "import os\n",
    "os.chdir('C:/Users/Domen/OneDrive/Projects/linkedIn-webscraper')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzlib\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmyModules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scraper_v2 \u001b[38;5;28;01mas\u001b[39;00m mybib \u001b[38;5;66;03m# importing own library\u001b[39;00m\n\u001b[0;32m     21\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Projects\\linkedIn-webscraper\\myModules\\scraper_v2.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m  \n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m \n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m  \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import winsound\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import json\n",
    "import zlib\n",
    "\n",
    "\n",
    "from myModules import scraper_v2 as mybib # importing own library\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/temp_data/\n",
      "data/webscrap_data/\n",
      "data/sound/\n"
     ]
    }
   ],
   "source": [
    "# load paths from config \n",
    "\n",
    "# print(os.getcwd())\n",
    "with open(\"config/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "temp_data_path = config[\"temp_data_path\"]\n",
    "webscrap_data_path = config[\"webscrap_data_path\"]   \n",
    "sound_path = config[\"sound_path\"]\n",
    "\n",
    "# check paths\n",
    "print(temp_data_path)\n",
    "print(webscrap_data_path)\n",
    "print(sound_path)\n",
    "\n",
    "soup_file =  'soup_dict'\n",
    "keyword_file = 'keyword_dict'\n",
    "database_file = 'database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tracker = {}\n",
    "with open(f\"{webscrap_data_path}id_tracker.pkl\", \"wb\") as file:\n",
    "                pickle.dump(id_tracker, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_analyst']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_list= ['data_analyst']\n",
    "keyword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error occurred: Ran out of input - soup_dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwebscrap_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msoup_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 3\u001b[0m     soup_dict \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msoup_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_backup.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(soup_dict, file)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - soup_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)      \n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwebscrap_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mkeyword_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mValueError\u001b[0m: Error occurred: Ran out of input - soup_dict"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     with open(f\"{webscrap_data_path}{soup_file}.pkl\", \"rb\") as file:\n",
    "#         soup_dict = pickle.load(file)\n",
    "#     with open(f\"{temp_data_path}{soup_file}_backup.pkl\", \"wb\") as file:\n",
    "#         pickle.dump(soup_dict, file)\n",
    "# except Exception as e:\n",
    "#     raise ValueError(f\"Error occurred: {e} - soup_dict\")      \n",
    "        \n",
    "# try:\n",
    "#     with open(f\"{webscrap_data_path}{keyword_file}.pkl\", \"rb\") as file:\n",
    "#         keyword_dict = pickle.load(file)\n",
    "#     with open(f\"{temp_data_path}{keyword_file}_backup.pkl\", \"wb\") as file:\n",
    "#         pickle.dump(soup_dict, file) \n",
    "# except Exception as e:\n",
    "#     raise ValueError(f\"Error occurred: {e} - keyword_dict\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/webscrap_data/\n",
      "soup_dict\n",
      "<_io.BufferedReader name='data/webscrap_data/soup_dict.pkl'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error occurred: Ran out of input - soup_dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(file)\n\u001b[1;32m----> 6\u001b[0m         soup_dict \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m         soup_dict \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - soup_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \n",
      "\u001b[1;31mValueError\u001b[0m: Error occurred: Ran out of input - soup_dict"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(f\"{webscrap_data_path}{soup_file}.pkl\", \"rb\") as file:\n",
    "        print(webscrap_data_path)\n",
    "        print(soup_file)\n",
    "        print(file)\n",
    "        soup_dict = pickle.load(file)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error occurred: {e} - soup_dict\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43msoup_dict\u001b[49m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# print(len(keyword_dict))\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup_dict' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(soup_dict))\n",
    "# print(len(keyword_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msoup_dict\u001b[49m)\n\u001b[0;32m      2\u001b[0m id_control \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m soup_dict\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup_dict' is not defined"
     ]
    }
   ],
   "source": [
    "print(soup_dict)\n",
    "id_control = []\n",
    "for item in soup_dict.keys():\n",
    "    id_control.append(item)\n",
    "\n",
    "# id_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL TIME BERLIN: https://www.linkedin.com/jobs/search?keywords={key}t&location={location}&locationId=&geoId={geo_id}_TPR=&distance=25&f_E={E}&position=1&pageNum=0\n",
    "# Past 24h Berlin: https://www.linkedin.com/jobs/search?keywords={key}t&location=Berlin%2C%20Berlin%2C%20Germany&locationId=&geoId=106967730&f_TPR=r86400&distance=25&f_E=2%2C3%2C4&position=1&pageNum=0\n",
    "# Last Week Berlin: https://www.linkedin.com/jobs/search?keywords={key}&location=Berlin%2C%20Berlin%2C%20Germany&locationId=&geoId=106967730&f_TPR=r604800&distance=25&f_E=2%2C3%2C4&position=1&pageNum=0\n",
    "# Last Month Berlin: https://www.linkedin.com/jobs/search?keywords=Data%20Analyst&location=Berlin%2C%20Berlin%2C%20Germany&locationId=&geoId=106967730&f_TPR=r2592000&distance=25&f_E=2%2C3%2C4&position=1&pageNum=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating backend links for data_analyst. Total of found pages: 40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:48<00:00,  1.22s/it]\n",
      "id 3807316776:   0%|                                                                  | 1/821 [00:00<00:00, 999.83it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error occurred: name 'id_control' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mid_control\u001b[49m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'id_control' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     64\u001b[0m     winsound\u001b[38;5;241m.\u001b[39mPlaySound(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msound_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msound1.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, winsound\u001b[38;5;241m.\u001b[39mSND_FILENAME)\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m winsound\u001b[38;5;241m.\u001b[39mPlaySound(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msound_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msound1.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, winsound\u001b[38;5;241m.\u001b[39mSND_FILENAME)\n",
      "\u001b[1;31mValueError\u001b[0m: Error occurred: name 'id_control' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    for key in keyword_list:\n",
    "        first_url = f\"https://www.linkedin.com/jobs/search?keywords={key}&location=Berlin%2C%20Berlin%2C%20Germany&locationId=&geoId=106967730&f_TPR=r604800&distance=25&f_E=2%2C3%2C4&position=1&pageNum=0\"\n",
    "        key_name = key.replace('%20', \" \")\n",
    "            \n",
    "        response = requests.get(first_url) # first request for keyword\n",
    "        response.status_code # 200 status code means OK!\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        number_of_results = soup.find('span', class_=\"results-context-header__job-count\").text # check number of searching results\n",
    "        numb = int(number_of_results.replace(\",\", \"\").replace(\"+\", \"\"))\n",
    "\n",
    "        backend_call_url_list = []\n",
    "        backend_call_url_list = mybib.create_backend_links(first_url, numb, key_name) # create list with sublinks to select different pages \n",
    "        \n",
    "        \n",
    "        \n",
    "        id_array = mybib.get_id_list(backend_call_url_list) # get job id's from all pages\n",
    "        \n",
    "\n",
    "        with tqdm(total=len(id_array), desc=\"Starting\") as pbar:\n",
    "        \n",
    "            for id in id_array:\n",
    "                counter = 0\n",
    "                backupsave = [25, 50, 75, 100, 125, 150, 175, 200, 300] \n",
    "                if counter in backupsave:\n",
    "                    new_dataframe = mybib.export_data(soup_dict, keyword_dict)\n",
    "                    scraper_df = pd.DataFrame(columns=scraper_df.columns)\n",
    "                    print(\"Updated dataframe\")\n",
    "                dynamic_text = f\"id {id}\" # text for tqdm progress bar status\n",
    "                pbar.set_description(dynamic_text) # change text\n",
    "                pbar.update(1)\n",
    "                \n",
    "                if str(id) not in id_control:\n",
    "                    try:\n",
    "                        \n",
    "                        response = requests.get(f'https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{id}')\n",
    "                        wait_time = randint(1,200)\n",
    "                        sleep(wait_time/1000)\n",
    "                        \n",
    "                        dict2 = {}\n",
    "                        dict2[\"scrap_date\"] = date.today()\n",
    "                        dict2[\"response\"] = response\n",
    "                        soup_dict[id] = dict2\n",
    "                        # try:\n",
    "                        #     keyword_dict[id].append(key_name)\n",
    "                        # except:\n",
    "                        #     keyword_dict[id] = [key_name]\n",
    "\n",
    "\n",
    "                    except Exception as e:\n",
    "                        raise ValueError(f\"Error occurred: {e} getting data from id {id}\")\n",
    "                        \n",
    "                else:\n",
    "                    pbar.set_description(f\"Will skip {id} because is already in the dataset.\")\n",
    "                    try:\n",
    "                        keyword_dict[id].append(key_name)\n",
    "                    except:\n",
    "                        keyword_dict[id] = [key_name]\n",
    "                counter += 1    \n",
    "        new_dataframe = mybib.export_data(soup_dict, keyword_dict)\n",
    "        print(\"\\n \\n\")\n",
    "except Exception as e:\n",
    "    winsound.PlaySound(f\"{sound_path}sound1.wav\", winsound.SND_FILENAME)\n",
    "    raise ValueError(f\"Error occurred: {e}\")\n",
    "    \n",
    "winsound.PlaySound(f\"{sound_path}sound1.wav\", winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(soup_dict))\n",
    "display(len(keyword_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe['id'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = new_dataframe.drop_duplicates(subset=['id'])\n",
    "df_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"webscraper/webscrap_data/{database_file}.pkl\" , \"wb\") as f:\n",
    "    pickle.dump(df_unique, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = new_dataframe.columns\n",
    "\n",
    "for col in columns:\n",
    "    null = new_dataframe[col].isna().sum()\n",
    "    print(f\"{round(null / len(new_dataframe[col]) * 100,1)}%    {col}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_input = ['Data%20Analyst']\n",
    "keywords = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_url = f\"https://www.linkedin.com/jobs/search?keywords=%7Bkey%7D&location=Berlin%2C%20Berlin%2C%20Germany&locationId=&geoId=106967730&f_TPR=&f_PP=106967730&distance=25&f_JT=F&f_E=2%2C3%2C4&position=1&pageNum=0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = requests.get(f'https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3717202675')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = soup_dict['3719620327']\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"section\",{\"class\":\"show-more-less-html\"}).text.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
