{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change direction to main folder to use path from config.json\n",
    "\n",
    "import os\n",
    "os.chdir('C:/Users/Domen/OneDrive/Projects/linkedIn-webscraper')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import winsound\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import json\n",
    "import zlib\n",
    "\n",
    "\n",
    "from myModules import scraper as mybib # importing own library\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load paths from config \n",
    "\n",
    "# print(os.getcwd())\n",
    "with open(\"config/config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "temp_data_path = config[\"temp_data_path\"]\n",
    "webscrap_data_path = config[\"webscrap_data_path\"]   \n",
    "sound_path = config[\"sound_path\"]\n",
    "\n",
    "# check paths\n",
    "print(temp_data_path)\n",
    "print(webscrap_data_path)\n",
    "print(sound_path)\n",
    "\n",
    "soup_file =  'soup_dict'\n",
    "keyword_file = 'keyword_dict'\n",
    "database_file = 'database'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list= ['data_analyst']\n",
    "keyword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(f\"{webscrap_data_path}{soup_file}.pkl\", \"rb\") as file:\n",
    "        soup_dict = pickle.load(file)\n",
    "    with open(f\"{temp_data_path}{soup_file}_backup.pkl\", \"wb\") as file:\n",
    "        pickle.dump(soup_dict, file)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error occurred: {e} - soup_dict\")      \n",
    "        \n",
    "# try:\n",
    "#     with open(f\"{webscrap_data_path}{keyword_file}.pkl\", \"rb\") as file:\n",
    "#         keyword_dict = pickle.load(file)\n",
    "#     with open(f\"{temp_data_path}{keyword_file}_backup.pkl\", \"wb\") as file:\n",
    "#         pickle.dump(soup_dict, file) \n",
    "# except Exception as e:\n",
    "#     raise ValueError(f\"Error occurred: {e} - keyword_dict\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(soup_dict))\n",
    "print(len(keyword_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_dict)\n",
    "id_control = []\n",
    "for item in soup_dict.keys():\n",
    "    id_control.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    for key in keyword_list:\n",
    "        first_url = f\"https://www.linkedin.com/jobs/search?keywords={key}&location=Stuttgart%2C%20Baden-W%C3%BCrttemberg%2C%20Germany&geoId=102473731&position=1&pageNum=0\"\n",
    "        key_name = key.replace('%20', \" \")\n",
    "            \n",
    "        response = requests.get(first_url) # first request for keyword\n",
    "        response.status_code # 200 status code means OK!\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        number_of_results = soup.find('span', class_=\"results-context-header__job-count\").text # check number of searching results\n",
    "        numb = int(number_of_results.replace(\",\", \"\").replace(\"+\", \"\"))\n",
    "\n",
    "        backend_call_url_list = []\n",
    "        backend_call_url_list = mybib.create_backend_links(first_url, numb, key_name) # create list with sublinks to select different pages \n",
    "        \n",
    "        \n",
    "        \n",
    "        id_array = mybib.get_id_list(backend_call_url_list) # get job id's from all pages\n",
    "        \n",
    "\n",
    "        with tqdm(total=len(id_array), desc=\"Starting\") as pbar:\n",
    "        \n",
    "            for id in id_array:\n",
    "                counter = 0\n",
    "                backupsave = [25, 50, 75, 100, 125, 150, 175, 200, 300] \n",
    "                if counter in backupsave:\n",
    "                    new_dataframe = mybib.export_data(soup_dict, keyword_dict)\n",
    "                    scraper_df = pd.DataFrame(columns=scraper_df.columns)\n",
    "                    print(\"Updated dataframe\")\n",
    "                dynamic_text = f\"id {id}\" # text for tqdm progress bar status\n",
    "                pbar.set_description(dynamic_text) # change text\n",
    "                pbar.update(1)\n",
    "                \n",
    "                if str(id) not in id_control:\n",
    "                    try:\n",
    "                        \n",
    "                        response = requests.get(f'https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{id}')\n",
    "                        wait_time = randint(1,200)\n",
    "                        sleep(wait_time/1000)\n",
    "                        \n",
    "                        dict2 = {}\n",
    "                        dict2[\"scrap_date\"] = date.today()\n",
    "                        dict2[\"response\"] = response\n",
    "                        soup_dict[id] = dict2\n",
    "                        try:\n",
    "                            keyword_dict[id].append(key_name)\n",
    "                        except:\n",
    "                            keyword_dict[id] = [key_name]\n",
    "\n",
    "\n",
    "                    except Exception as e:\n",
    "                        raise ValueError(f\"Error occurred: {e} getting data from id {id}\")\n",
    "                        \n",
    "                else:\n",
    "                    pbar.set_description(f\"Will skip {id} because is already in the dataset.\")\n",
    "                    try:\n",
    "                        keyword_dict[id].append(key_name)\n",
    "                    except:\n",
    "                        keyword_dict[id] = [key_name]\n",
    "                counter += 1    \n",
    "        new_dataframe = mybib.export_data(soup_dict, keyword_dict)\n",
    "        print(\"\\n \\n\")\n",
    "except Exception as e:\n",
    "    winsound.PlaySound(f\"{sound_path}alarm3.wav\", winsound.SND_FILENAME)\n",
    "    raise ValueError(f\"Error occurred: {e}\")\n",
    "    \n",
    "winsound.PlaySound(f\"{sound_path}alarm3.wav\", winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(soup_dict))\n",
    "display(len(keyword_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe['id'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = new_dataframe.drop_duplicates(subset=['id'])\n",
    "df_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"webscraper/webscrap_data/{database_file}.pkl\" , \"wb\") as f:\n",
    "    pickle.dump(df_unique, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = new_dataframe.columns\n",
    "\n",
    "for col in columns:\n",
    "    null = new_dataframe[col].isna().sum()\n",
    "    print(f\"{round(null / len(new_dataframe[col]) * 100,1)}%    {col}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_input = ['Data%20Analyst']\n",
    "keywords = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_url = f\"https://www.linkedin.com/jobs/search?keywords=%7Bkey%7D&location=Berlin%2C%20Berlin%2C%20Germany&locationId=&geoId=106967730&f_TPR=&f_PP=106967730&distance=25&f_JT=F&f_E=2%2C3%2C4&position=1&pageNum=0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = requests.get(f'https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3717202675')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_dict['3719620327']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = soup_dict['3719620327']\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"section\",{\"class\":\"show-more-less-html\"}).text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_soup_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in soup_dict:\n",
    "    temp = {}\n",
    "    \n",
    "    dict2 = {}\n",
    "    dict2[\"scrap_date\"] = date.today()\n",
    "    dict2[\"response\"] = soup_dict[key]\n",
    "    new_soup_dict[key] = dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_soup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"webscraper/webscrap_data/soup_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(new_soup_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = mybib.get_id_list(backend_call_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
